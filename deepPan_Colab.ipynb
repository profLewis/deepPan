{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ deepPan Steel Pan Synthesizer\n",
    "\n",
    "Interactive steel pan sound design in Google Colab.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/profLewis/deepPan/blob/main/deepPan_Colab.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numpy scipy ipywidgets\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import IPython.display as ipd\n",
    "from ipywidgets import interact, interactive, FloatSlider, IntSlider, Dropdown, Button, Output, HBox, VBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import json\n",
    "\n",
    "print(\"‚úì Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note frequencies and mapping\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "NOTE_FREQUENCIES = {\n",
    "    'C': 261.63, 'C#': 277.18, 'Db': 277.18,\n",
    "    'D': 293.66, 'D#': 311.13, 'Eb': 311.13,\n",
    "    'E': 329.63, 'F': 349.23, 'F#': 369.99,\n",
    "    'G': 392.00, 'G#': 415.30, 'Ab': 415.30,\n",
    "    'A': 440.00, 'A#': 466.16, 'Bb': 466.16,\n",
    "    'B': 493.88,\n",
    "}\n",
    "\n",
    "# All 29 notes on the tenor pan\n",
    "TENOR_PAN_NOTES = [\n",
    "    # Outer ring (octave 4)\n",
    "    'F#4', 'B4', 'E4', 'A4', 'D4', 'G4', 'C4', 'F4', 'Bb4', 'Eb4', 'Ab4', 'C#4',\n",
    "    # Central ring (octave 5)\n",
    "    'F#5', 'B5', 'E5', 'A5', 'D5', 'G5', 'C5', 'F5', 'Bb5', 'Eb5', 'Ab5', 'C#5',\n",
    "    # Inner ring (octave 6)\n",
    "    'C#6', 'E6', 'D6', 'C6', 'Eb6'\n",
    "]\n",
    "\n",
    "def get_frequency(note_str):\n",
    "    \"\"\"Parse note string like 'C4' or 'F#5' and return frequency.\"\"\"\n",
    "    if len(note_str) >= 2:\n",
    "        if len(note_str) >= 3 and note_str[1] in '#b':\n",
    "            note_name = note_str[:2]\n",
    "            octave = int(note_str[2:])\n",
    "        else:\n",
    "            note_name = note_str[0]\n",
    "            octave = int(note_str[1:])\n",
    "        base_freq = NOTE_FREQUENCIES.get(note_name, 440)\n",
    "        return base_freq * (2 ** (octave - 4))\n",
    "    return 440\n",
    "\n",
    "print(f\"‚úì Tenor pan has {len(TENOR_PAN_NOTES)} notes\")\n",
    "print(f\"  Outer ring (4): {', '.join(TENOR_PAN_NOTES[:12])}\")\n",
    "print(f\"  Central ring (5): {', '.join(TENOR_PAN_NOTES[12:24])}\")\n",
    "print(f\"  Inner ring (6): {', '.join(TENOR_PAN_NOTES[24:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "params = {\n",
    "    'attack': 15,       # ms\n",
    "    'decay': 500,       # ms\n",
    "    'sustain': 20,      # %\n",
    "    'release': 300,     # ms\n",
    "    'fundamental': 100, # %\n",
    "    'harmonic2': 30,    # %\n",
    "    'harmonic3': 10,    # %\n",
    "    'harmonic4': 5,     # %\n",
    "    'sub_bass': 20,     # %\n",
    "    'detune': 2,        # cents\n",
    "    'filter': 6000,     # Hz\n",
    "    'brightness': 50,   # %\n",
    "    'duration': 1.5,    # seconds\n",
    "    'volume': 85,       # %\n",
    "}\n",
    "\n",
    "# Presets\n",
    "PRESETS = {\n",
    "    'default': dict(attack=15, decay=500, sustain=20, release=300,\n",
    "                    fundamental=100, harmonic2=30, harmonic3=10, harmonic4=5,\n",
    "                    sub_bass=20, detune=2, filter=6000, brightness=50, duration=1.5, volume=85),\n",
    "    'bright': dict(attack=5, decay=300, sustain=10, release=200,\n",
    "                   fundamental=80, harmonic2=50, harmonic3=30, harmonic4=20,\n",
    "                   sub_bass=10, detune=3, filter=10000, brightness=80, duration=1.5, volume=85),\n",
    "    'mellow': dict(attack=30, decay=800, sustain=30, release=500,\n",
    "                   fundamental=100, harmonic2=15, harmonic3=5, harmonic4=2,\n",
    "                   sub_bass=30, detune=1, filter=3000, brightness=30, duration=1.5, volume=85),\n",
    "    'bell': dict(attack=2, decay=1500, sustain=5, release=1000,\n",
    "                 fundamental=70, harmonic2=60, harmonic3=40, harmonic4=30,\n",
    "                 sub_bass=5, detune=5, filter=8000, brightness=60, duration=1.5, volume=85),\n",
    "    'pluck': dict(attack=1, decay=200, sustain=0, release=100,\n",
    "                  fundamental=100, harmonic2=40, harmonic3=20, harmonic4=10,\n",
    "                  sub_bass=15, detune=0, filter=5000, brightness=50, duration=1.5, volume=85),\n",
    "}\n",
    "\n",
    "print(\"‚úì Parameters initialized\")\n",
    "print(f\"  Presets: {', '.join(PRESETS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_note(frequency, params):\n",
    "    \"\"\"Generate a steel pan note with given parameters.\"\"\"\n",
    "    duration = params['duration']\n",
    "    t = np.linspace(0, duration, int(SAMPLE_RATE * duration), endpoint=False)\n",
    "    \n",
    "    # Convert parameters\n",
    "    attack_time = params['attack'] / 1000\n",
    "    decay_time = params['decay'] / 1000\n",
    "    sustain_level = params['sustain'] / 100\n",
    "    release_time = params['release'] / 1000\n",
    "    detune_cents = params['detune']\n",
    "    \n",
    "    # Partial structure\n",
    "    partials = [\n",
    "        (0.50, params['sub_bass'] / 100, 1.0),\n",
    "        (1.00 - detune_cents/1000, params['fundamental'] / 100 * 0.1, 1.0),\n",
    "        (1.00, params['fundamental'] / 100, 1.0),\n",
    "        (1.00 + detune_cents/1000, params['fundamental'] / 100 * 0.1, 1.0),\n",
    "        (2.00, params['harmonic2'] / 100, 1.3),\n",
    "        (3.00, params['harmonic3'] / 100, 1.8),\n",
    "        (4.00, params['harmonic4'] / 100, 2.2),\n",
    "    ]\n",
    "    \n",
    "    attack_samples = int(attack_time * SAMPLE_RATE)\n",
    "    decay_samples = int(decay_time * SAMPLE_RATE)\n",
    "    \n",
    "    sound = np.zeros_like(t)\n",
    "    \n",
    "    for ratio, amp, decay_mult in partials:\n",
    "        if amp < 0.01:\n",
    "            continue\n",
    "        \n",
    "        freq = frequency * ratio\n",
    "        if freq > SAMPLE_RATE / 2 - 500 or freq < 30:\n",
    "            continue\n",
    "        \n",
    "        # ADSR envelope\n",
    "        env = np.ones_like(t)\n",
    "        \n",
    "        if attack_samples > 0:\n",
    "            attack = np.sin(np.linspace(0, np.pi/2, attack_samples)) ** 1.5\n",
    "            env[:attack_samples] = attack\n",
    "        \n",
    "        decay_start = attack_samples\n",
    "        decay_end = min(decay_start + decay_samples, len(t))\n",
    "        if decay_end > decay_start:\n",
    "            decay = np.linspace(1.0, sustain_level, decay_end - decay_start)\n",
    "            env[decay_start:decay_end] = decay\n",
    "        \n",
    "        sustain_start = decay_end\n",
    "        sustain_tau = decay_time / decay_mult\n",
    "        if sustain_start < len(t):\n",
    "            env[sustain_start:] = sustain_level * np.exp(\n",
    "                -(t[sustain_start:] - t[sustain_start]) / sustain_tau\n",
    "            )\n",
    "        \n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        partial_sound = amp * env * np.sin(2 * np.pi * freq * t + phase)\n",
    "        sound += partial_sound\n",
    "    \n",
    "    # Low-pass filter\n",
    "    nyq = SAMPLE_RATE / 2\n",
    "    cutoff = min(params['filter'], nyq - 100) / nyq\n",
    "    if 0.01 < cutoff < 0.99:\n",
    "        b, a = signal.butter(2, cutoff, btype='low')\n",
    "        sound = signal.filtfilt(b, a, sound)\n",
    "    \n",
    "    # Brightness adjustment\n",
    "    if params['brightness'] != 50:\n",
    "        brightness_boost = (params['brightness'] - 50) / 50\n",
    "        if brightness_boost > 0:\n",
    "            high_cutoff = 2000 / nyq\n",
    "            if high_cutoff < 0.99:\n",
    "                b, a = signal.butter(1, high_cutoff, btype='high')\n",
    "                highs = signal.filtfilt(b, a, sound)\n",
    "                sound += highs * brightness_boost * 0.5\n",
    "    \n",
    "    # Soft limiting and normalize\n",
    "    sound = np.tanh(sound * 1.2) / 1.2\n",
    "    max_val = np.max(np.abs(sound))\n",
    "    if max_val > 0:\n",
    "        sound = sound / max_val * (params['volume'] / 100)\n",
    "    \n",
    "    return sound\n",
    "\n",
    "print(\"‚úì Synthesis engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_note(note, params):\n",
    "    \"\"\"Generate and play a note.\"\"\"\n",
    "    freq = get_frequency(note)\n",
    "    audio = generate_note(freq, params)\n",
    "    return ipd.Audio(audio, rate=SAMPLE_RATE, autoplay=True)\n",
    "\n",
    "def play_sequence(notes, params, bpm=120):\n",
    "    \"\"\"Generate a sequence of notes.\"\"\"\n",
    "    note_duration = 60 / bpm\n",
    "    silence_samples = int(note_duration * SAMPLE_RATE)\n",
    "    \n",
    "    all_audio = []\n",
    "    for note in notes.split():\n",
    "        if note in (',,', '-', 'r', '.'):\n",
    "            # Rest\n",
    "            all_audio.append(np.zeros(silence_samples))\n",
    "        else:\n",
    "            freq = get_frequency(note)\n",
    "            audio = generate_note(freq, params)\n",
    "            # Trim or pad to beat length\n",
    "            if len(audio) > silence_samples:\n",
    "                audio = audio[:silence_samples]\n",
    "            else:\n",
    "                audio = np.pad(audio, (0, silence_samples - len(audio)))\n",
    "            all_audio.append(audio)\n",
    "    \n",
    "    combined = np.concatenate(all_audio)\n",
    "    return ipd.Audio(combined, rate=SAMPLE_RATE, autoplay=True)\n",
    "\n",
    "print(\"‚úì Playback functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéπ Quick Play\n",
    "\n",
    "Play individual notes or sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a single note\n",
    "play_note('C5', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a scale\n",
    "play_sequence('C4 D4 E4 F4 G4 A4 B4 C5', params, bpm=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a melody\n",
    "play_sequence('E5 E5 F5 G5 G5 F5 E5 D5 C5 C5 D5 E5 E5 ,, D5 D5', params, bpm=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéõÔ∏è Interactive Synthesizer\n",
    "\n",
    "Adjust parameters with sliders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create interactive synthesizer\noutput = Output()\n\n# Sliders\nnote_dropdown = Dropdown(options=TENOR_PAN_NOTES, value='C5', description='Note:')\npreset_dropdown = Dropdown(options=list(PRESETS.keys()), value='default', description='Preset:')\n\nattack_slider = IntSlider(min=1, max=200, value=params['attack'], description='Attack (ms)')\ndecay_slider = IntSlider(min=50, max=2000, value=params['decay'], description='Decay (ms)')\nsustain_slider = IntSlider(min=0, max=100, value=params['sustain'], description='Sustain (%)')\nrelease_slider = IntSlider(min=50, max=2000, value=params['release'], description='Release (ms)')\n\nfundamental_slider = IntSlider(min=0, max=100, value=params['fundamental'], description='Fundamental')\nharmonic2_slider = IntSlider(min=0, max=100, value=params['harmonic2'], description='Harmonic 2')\nharmonic3_slider = IntSlider(min=0, max=100, value=params['harmonic3'], description='Harmonic 3')\nharmonic4_slider = IntSlider(min=0, max=100, value=params['harmonic4'], description='Harmonic 4')\nsub_bass_slider = IntSlider(min=0, max=100, value=params['sub_bass'], description='Sub Bass')\n\ndetune_slider = IntSlider(min=0, max=20, value=params['detune'], description='Detune (ct)')\nfilter_slider = IntSlider(min=500, max=10000, value=params['filter'], description='Filter (Hz)')\nbrightness_slider = IntSlider(min=0, max=100, value=params['brightness'], description='Brightness')\nvolume_slider = IntSlider(min=0, max=100, value=params['volume'], description='Volume (%)')\n\ndef update_params():\n    params['attack'] = attack_slider.value\n    params['decay'] = decay_slider.value\n    params['sustain'] = sustain_slider.value\n    params['release'] = release_slider.value\n    params['fundamental'] = fundamental_slider.value\n    params['harmonic2'] = harmonic2_slider.value\n    params['harmonic3'] = harmonic3_slider.value\n    params['harmonic4'] = harmonic4_slider.value\n    params['sub_bass'] = sub_bass_slider.value\n    params['detune'] = detune_slider.value\n    params['filter'] = filter_slider.value\n    params['brightness'] = brightness_slider.value\n    params['volume'] = volume_slider.value\n\ndef load_preset(change):\n    preset = PRESETS[preset_dropdown.value]\n    attack_slider.value = preset['attack']\n    decay_slider.value = preset['decay']\n    sustain_slider.value = preset['sustain']\n    release_slider.value = preset['release']\n    fundamental_slider.value = preset['fundamental']\n    harmonic2_slider.value = preset['harmonic2']\n    harmonic3_slider.value = preset['harmonic3']\n    harmonic4_slider.value = preset['harmonic4']\n    sub_bass_slider.value = preset['sub_bass']\n    detune_slider.value = preset['detune']\n    filter_slider.value = preset['filter']\n    brightness_slider.value = preset['brightness']\n    volume_slider.value = preset['volume']\n\npreset_dropdown.observe(load_preset, names='value')\n\ndef play_current(b):\n    update_params()\n    with output:\n        clear_output(wait=True)\n        display(play_note(note_dropdown.value, params))\n\nplay_btn = Button(description='Play Note', button_style='success')\nplay_btn.on_click(play_current)\n\n# Layout - use widgets.HTML for ipywidgets containers\ncontrols = VBox([\n    widgets.HTML('<h3>Note Selection</h3>'),\n    HBox([note_dropdown, preset_dropdown, play_btn]),\n    widgets.HTML('<h3>Envelope (ADSR)</h3>'),\n    HBox([attack_slider, decay_slider]),\n    HBox([sustain_slider, release_slider]),\n    widgets.HTML('<h3>Harmonics</h3>'),\n    HBox([fundamental_slider, harmonic2_slider]),\n    HBox([harmonic3_slider, harmonic4_slider]),\n    sub_bass_slider,\n    widgets.HTML('<h3>Character</h3>'),\n    HBox([detune_slider, filter_slider]),\n    HBox([brightness_slider, volume_slider]),\n    widgets.HTML('<h3>Output</h3>'),\n    output\n])\n\ndisplay(controls)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Save / Load Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current parameters to JSON\n",
    "def save_params(filename='synth_params.json'):\n",
    "    update_params()\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    print(f\"‚úì Saved parameters to {filename}\")\n",
    "    print(json.dumps(params, indent=2))\n",
    "\n",
    "# Load parameters from JSON\n",
    "def load_params(filename='synth_params.json'):\n",
    "    global params\n",
    "    with open(filename, 'r') as f:\n",
    "        loaded = json.load(f)\n",
    "    params.update(loaded)\n",
    "    load_preset(None)  # Update sliders\n",
    "    print(f\"‚úì Loaded parameters from {filename}\")\n",
    "\n",
    "# Example: save current settings\n",
    "save_params('my_sound.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CLI command for generate_sounds.py\n",
    "def get_cli_command():\n",
    "    update_params()\n",
    "    defaults = PRESETS['default']\n",
    "    cmd = 'python generate_sounds.py'\n",
    "    \n",
    "    for key, value in params.items():\n",
    "        if value != defaults.get(key):\n",
    "            cli_key = key.replace('_', '-')\n",
    "            cmd += f' --{cli_key} {value}'\n",
    "    \n",
    "    return cmd\n",
    "\n",
    "print(\"Equivalent CLI command:\")\n",
    "print(get_cli_command())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé∂ Sequence Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive sequence player\nseq_output = Output()\n\nsequence_input = widgets.Text(\n    value='C4 E4 G4 C5 G4 E4 C4',\n    description='Sequence:',\n    layout=widgets.Layout(width='80%')\n)\n\nbpm_slider = IntSlider(min=60, max=200, value=120, description='BPM:')\n\ndef play_seq(b):\n    update_params()\n    with seq_output:\n        clear_output(wait=True)\n        display(play_sequence(sequence_input.value, params, bpm_slider.value))\n\nplay_seq_btn = Button(description='Play Sequence', button_style='success')\nplay_seq_btn.on_click(play_seq)\n\ndisplay(VBox([\n    sequence_input,\n    HBox([bpm_slider, play_seq_btn]),\n    widgets.HTML('<p style=\"color: gray; font-size: 12px;\">Notes: C4-B4 (outer), C5-B5 (central), C6-Eb6 (inner) | Rest: ,,</p>'),\n    seq_output\n]))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Audio Analysis\n\nUpload a WAV file to analyze and extract synthesis parameters:"
  },
  {
   "cell_type": "code",
   "source": "from scipy.fft import fft, fftfreq\n\nNOTE_NAMES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n\ndef freq_to_note(freq):\n    \"\"\"Convert frequency to note name and octave.\"\"\"\n    if freq <= 0:\n        return None, None\n    midi_note = int(round(69 + 12 * np.log2(freq / 440.0)))\n    octave = (midi_note // 12) - 1\n    note_idx = midi_note % 12\n    return NOTE_NAMES[note_idx], octave\n\ndef detect_pitch(audio, sample_rate, min_freq=50, max_freq=2000):\n    \"\"\"Detect fundamental frequency using autocorrelation and FFT.\"\"\"\n    # Use a segment from the sustain portion\n    seg_start = int(0.05 * sample_rate)\n    seg_length = int(0.2 * sample_rate)\n    seg_end = min(seg_start + seg_length, len(audio))\n    \n    if seg_end <= seg_start:\n        segment = audio\n    else:\n        segment = audio[seg_start:seg_end].copy()\n    \n    # Normalize\n    segment = segment - np.mean(segment)\n    if np.max(np.abs(segment)) > 0:\n        segment = segment / np.max(np.abs(segment))\n    \n    # Autocorrelation method\n    min_lag = int(sample_rate / max_freq)\n    max_lag = min(int(sample_rate / min_freq), len(segment) - 1)\n    \n    corr = np.correlate(segment, segment, mode='full')\n    corr = corr[len(corr)//2:]\n    \n    if max_lag > len(corr):\n        max_lag = len(corr) - 1\n    \n    corr_segment = corr[min_lag:max_lag]\n    if len(corr_segment) == 0:\n        return 0\n    \n    peak_idx = np.argmax(corr_segment) + min_lag\n    freq_autocorr = sample_rate / peak_idx if peak_idx > 0 else 0\n    \n    # FFT method with harmonic product spectrum\n    n = len(segment)\n    window = np.hanning(n)\n    spectrum = np.abs(fft(segment * window))[:n//2]\n    freqs = fftfreq(n, 1/sample_rate)[:n//2]\n    \n    # Harmonic product spectrum\n    hps = spectrum.copy()\n    for h in range(2, 5):\n        decimated = spectrum[::h]\n        hps[:len(decimated)] *= decimated\n    \n    freq_mask = (freqs >= min_freq) & (freqs <= max_freq)\n    valid_hps = hps.copy()\n    valid_hps[~freq_mask] = 0\n    \n    peak_idx = np.argmax(valid_hps)\n    freq_fft = freqs[peak_idx] if peak_idx > 0 else 0\n    \n    # Use FFT if autocorrelation seems off\n    if abs(freq_fft - freq_autocorr) > freq_autocorr * 0.1:\n        return freq_fft\n    return (freq_autocorr + freq_fft) / 2\n\ndef analyze_harmonics(audio, sample_rate, fundamental_freq):\n    \"\"\"Analyze harmonic content relative to fundamental.\"\"\"\n    seg_start = int(0.1 * sample_rate)\n    seg_length = int(0.3 * sample_rate)\n    seg_end = min(seg_start + seg_length, len(audio))\n    \n    segment = audio[seg_start:seg_end] if seg_end > seg_start else audio\n    n = len(segment)\n    window = np.hanning(n)\n    spectrum = np.abs(fft(segment * window))[:n//2]\n    freqs = fftfreq(n, 1/sample_rate)[:n//2]\n    \n    harmonics = {}\n    harmonic_ratios = [0.5, 1.0, 2.0, 3.0, 4.0]\n    harmonic_names = ['sub_bass', 'fundamental', 'harmonic2', 'harmonic3', 'harmonic4']\n    \n    fund_amp = 0\n    for ratio, name in zip(harmonic_ratios, harmonic_names):\n        target_freq = fundamental_freq * ratio\n        tolerance = fundamental_freq * 0.05\n        freq_mask = (freqs >= target_freq - tolerance) & (freqs <= target_freq + tolerance)\n        \n        if np.any(freq_mask):\n            amp = np.max(spectrum[freq_mask])\n            harmonics[name] = amp\n            if name == 'fundamental':\n                fund_amp = amp\n        else:\n            harmonics[name] = 0\n    \n    # Normalize to fundamental\n    if fund_amp > 0:\n        for name in harmonic_names:\n            harmonics[name] = min(100, int((harmonics[name] / fund_amp) * 100))\n    \n    return harmonics\n\ndef analyze_envelope(audio, sample_rate):\n    \"\"\"Estimate ADSR envelope parameters.\"\"\"\n    envelope = np.abs(audio)\n    window_size = int(0.01 * sample_rate)\n    if window_size > 1:\n        kernel = np.ones(window_size) / window_size\n        envelope = np.convolve(envelope, kernel, mode='same')\n    \n    max_amp = np.max(envelope)\n    if max_amp > 0:\n        envelope = envelope / max_amp\n    \n    # Attack\n    peak_idx = np.argmax(envelope)\n    attack_idx = 0\n    for i in range(peak_idx):\n        if envelope[i] >= 0.9:\n            attack_idx = i\n            break\n    attack_ms = (attack_idx / sample_rate) * 1000\n    \n    # Sustain level\n    sustain_start = int(0.3 * len(envelope))\n    sustain_end = int(0.6 * len(envelope))\n    sustain_level = np.mean(envelope[sustain_start:sustain_end]) if sustain_end > sustain_start else 0.2\n    \n    # Decay\n    decay_idx = peak_idx\n    for i in range(peak_idx, len(envelope)):\n        if envelope[i] <= sustain_level * 1.1:\n            decay_idx = i\n            break\n    decay_ms = ((decay_idx - peak_idx) / sample_rate) * 1000\n    \n    # Release\n    release_start = int(0.7 * len(envelope))\n    if release_start < len(envelope):\n        release_segment = envelope[release_start:]\n        threshold = release_segment[0] * 0.1 if len(release_segment) > 0 else 0\n        release_idx = len(release_segment)\n        for i, val in enumerate(release_segment):\n            if val <= threshold:\n                release_idx = i\n                break\n        release_ms = (release_idx / sample_rate) * 1000\n    else:\n        release_ms = 300\n    \n    return {\n        'attack': max(1, min(200, int(attack_ms))),\n        'decay': max(50, min(2000, int(decay_ms))),\n        'sustain': max(0, min(100, int(sustain_level * 100))),\n        'release': max(50, min(2000, int(release_ms)))\n    }\n\ndef estimate_filter_brightness(audio, sample_rate, fundamental_freq):\n    \"\"\"Estimate filter cutoff and brightness.\"\"\"\n    n = len(audio)\n    window = np.hanning(n)\n    spectrum = np.abs(fft(audio * window))[:n//2]\n    freqs = fftfreq(n, 1/sample_rate)[:n//2]\n    \n    # Spectral centroid\n    if np.sum(spectrum) > 0:\n        centroid = np.sum(freqs * spectrum) / np.sum(spectrum)\n    else:\n        centroid = 1000\n    \n    # Rolloff frequency (85% energy)\n    cumsum = np.cumsum(spectrum)\n    total = cumsum[-1]\n    if total > 0:\n        rolloff_idx = np.where(cumsum >= 0.85 * total)[0]\n        rolloff_freq = freqs[rolloff_idx[0]] if len(rolloff_idx) > 0 else 6000\n    else:\n        rolloff_freq = 6000\n    \n    filter_cutoff = max(500, min(10000, int(rolloff_freq)))\n    brightness_ratio = centroid / fundamental_freq if fundamental_freq > 0 else 2\n    brightness = max(0, min(100, int((brightness_ratio - 1) * 25 + 50)))\n    \n    return {'filter': filter_cutoff, 'brightness': brightness}\n\ndef analyze_audio_file(filepath):\n    \"\"\"Analyze an audio file and extract synthesis parameters.\"\"\"\n    sample_rate, audio = wavfile.read(filepath)\n    \n    # Convert to mono float\n    if len(audio.shape) > 1:\n        audio = np.mean(audio, axis=1)\n    if audio.dtype == np.int16:\n        audio = audio.astype(np.float32) / 32768.0\n    elif audio.dtype == np.int32:\n        audio = audio.astype(np.float32) / 2147483648.0\n    \n    # Detect pitch\n    fundamental_freq = detect_pitch(audio, sample_rate)\n    note_name, octave = freq_to_note(fundamental_freq)\n    detected_note = f\"{note_name}{octave}\" if note_name else \"Unknown\"\n    \n    # Check if on tenor pan\n    on_pan = detected_note in TENOR_PAN_NOTES\n    \n    # Analyze components\n    harmonics = analyze_harmonics(audio, sample_rate, fundamental_freq)\n    envelope = analyze_envelope(audio, sample_rate)\n    filter_bright = estimate_filter_brightness(audio, sample_rate, fundamental_freq)\n    \n    # Build params\n    extracted_params = {\n        'attack': envelope['attack'],\n        'decay': envelope['decay'],\n        'sustain': envelope['sustain'],\n        'release': envelope['release'],\n        'fundamental': harmonics.get('fundamental', 100),\n        'harmonic2': harmonics.get('harmonic2', 30),\n        'harmonic3': harmonics.get('harmonic3', 10),\n        'harmonic4': harmonics.get('harmonic4', 5),\n        'sub_bass': harmonics.get('sub_bass', 20),\n        'detune': 2,\n        'filter': filter_bright['filter'],\n        'brightness': filter_bright['brightness'],\n        'duration': len(audio) / sample_rate,\n        'volume': 85\n    }\n    \n    return {\n        'detected_note': detected_note,\n        'frequency': round(fundamental_freq, 2),\n        'on_pan': on_pan,\n        'params': extracted_params,\n        'audio': audio,\n        'sample_rate': sample_rate\n    }\n\nprint(\"Audio analysis functions ready\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Interactive audio analysis with file upload (supports multiple files)\nfrom google.colab import files\nimport matplotlib.pyplot as plt\n\nanalysis_output = Output()\nall_results = []  # Store multiple analysis results\nselected_idx = 0\n\ndef upload_and_analyze(b):\n    global all_results, selected_idx, params\n    with analysis_output:\n        clear_output(wait=True)\n        print(\"Upload WAV file(s)... (you can select multiple)\")\n        uploaded = files.upload()\n        \n        if not uploaded:\n            print(\"No files uploaded\")\n            return\n        \n        all_results = []\n        for filename in uploaded.keys():\n            print(f\"\\nAnalyzing {filename}...\")\n            try:\n                result = analyze_audio_file(filename)\n                result['filename'] = filename\n                all_results.append(result)\n            except Exception as e:\n                print(f\"  Error: {e}\")\n        \n        if not all_results:\n            print(\"No files could be analyzed\")\n            return\n        \n        # Print summary table\n        print(f\"\\n{'='*70}\")\n        print(f\"ANALYSIS SUMMARY ({len(all_results)} files)\")\n        print(f\"{'='*70}\")\n        print(f\"{'#':<3} {'File':<25} {'Note':<6} {'Freq':>8} {'On Pan':<6}\")\n        print(\"-\"*70)\n        \n        for i, r in enumerate(all_results):\n            fname = r['filename'][:23]\n            on_pan = \"Yes\" if r['on_pan'] else \"No\"\n            print(f\"{i:<3} {fname:<25} {r['detected_note']:<6} {r['frequency']:>8.1f} {on_pan:<6}\")\n        \n        print(\"-\"*70)\n        on_pan_count = sum(1 for r in all_results if r['on_pan'])\n        print(f\"Total: {len(all_results)} files, {on_pan_count} notes on tenor pan\")\n        \n        # Show first result details\n        selected_idx = 0\n        show_result_details(0)\n\ndef show_result_details(idx):\n    global selected_idx\n    selected_idx = idx\n    result = all_results[idx]\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"DETAILS: {result['filename']}\")\n    print(f\"{'='*50}\")\n    print(f\"Detected Note: {result['detected_note']}\")\n    print(f\"Frequency: {result['frequency']} Hz\")\n    print(f\"On Tenor Pan: {'Yes' if result['on_pan'] else 'No'}\")\n    \n    p = result['params']\n    print(f\"\\n--- Envelope (ADSR) ---\")\n    print(f\"Attack:  {p['attack']} ms\")\n    print(f\"Decay:   {p['decay']} ms\")\n    print(f\"Sustain: {p['sustain']}%\")\n    print(f\"Release: {p['release']} ms\")\n    \n    print(f\"\\n--- Harmonics ---\")\n    print(f\"Fundamental: {p['fundamental']}%\")\n    print(f\"Harmonic 2:  {p['harmonic2']}%\")\n    print(f\"Harmonic 3:  {p['harmonic3']}%\")\n    print(f\"Harmonic 4:  {p['harmonic4']}%\")\n    print(f\"Sub Bass:    {p['sub_bass']}%\")\n    \n    print(f\"\\n--- Character ---\")\n    print(f\"Filter:     {p['filter']} Hz\")\n    print(f\"Brightness: {p['brightness']}%\")\n    \n    # Plot waveform and spectrum\n    fig, axes = plt.subplots(2, 1, figsize=(10, 4))\n    \n    t = np.arange(len(result['audio'])) / result['sample_rate']\n    axes[0].plot(t, result['audio'], color='orange', linewidth=0.5)\n    axes[0].set_xlabel('Time (s)')\n    axes[0].set_ylabel('Amplitude')\n    axes[0].set_title(f'Waveform: {result[\"detected_note\"]} ({result[\"frequency\"]} Hz)')\n    axes[0].grid(True, alpha=0.3)\n    \n    n = min(8192, len(result['audio']))\n    spectrum = np.abs(fft(result['audio'][:n] * np.hanning(n)))[:n//2]\n    freqs = fftfreq(n, 1/result['sample_rate'])[:n//2]\n    axes[1].plot(freqs[:2000], spectrum[:2000], color='cyan', linewidth=0.5)\n    axes[1].axvline(result['frequency'], color='red', linestyle='--', label=f'F0={result[\"frequency\"]}Hz')\n    axes[1].set_xlabel('Frequency (Hz)')\n    axes[1].set_ylabel('Magnitude')\n    axes[1].set_title('Spectrum')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef apply_analysis(b):\n    global params\n    if not all_results:\n        print(\"No analysis result to apply\")\n        return\n    \n    result = all_results[selected_idx]\n    for key, value in result['params'].items():\n        if key in params:\n            params[key] = value\n    \n    load_preset(None)\n    print(f\"Parameters from {result['filename']} applied to synthesizer!\")\n    print(f\"Try playing {result['detected_note']} to compare\")\n\ndef play_original(b):\n    if not all_results:\n        print(\"No audio loaded\")\n        return\n    result = all_results[selected_idx]\n    with analysis_output:\n        display(ipd.Audio(result['audio'], rate=result['sample_rate'], autoplay=True))\n\n# File selector dropdown (for multiple files)\nfile_selector = Dropdown(options=[], description='File:')\n\ndef on_file_select(change):\n    if all_results and change['new'] is not None:\n        with analysis_output:\n            show_result_details(change['new'])\n\nfile_selector.observe(on_file_select, names='value')\n\nupload_btn = Button(description='Upload & Analyze WAV(s)', button_style='warning')\nupload_btn.on_click(upload_and_analyze)\n\napply_btn = Button(description='Apply to Synth', button_style='success')\napply_btn.on_click(apply_analysis)\n\nplay_orig_btn = Button(description='Play Original', button_style='info')\nplay_orig_btn.on_click(play_original)\n\ndisplay(VBox([\n    HBox([upload_btn, apply_btn, play_orig_btn]),\n    analysis_output\n]))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Download and analyze example steel pan sample from the web\nimport urllib.request\n\ndef download_sample(url, filename='sample.wav'):\n    \"\"\"Download a WAV sample from URL.\"\"\"\n    print(f\"Downloading {url}...\")\n    urllib.request.urlretrieve(url, filename)\n    print(f\"Saved to {filename}\")\n    return filename\n\n# Example: Download a steel drum sample from Free Wave Samples\n# (You can replace this URL with any WAV file URL)\nsample_url = \"https://freewavesamples.com/files/Steel-Drum-C4.wav\"\n\ntry:\n    local_file = download_sample(sample_url, 'steel_drum_sample.wav')\n    result = analyze_audio_file(local_file)\n    \n    print(f\"\\nDetected: {result['detected_note']} ({result['frequency']} Hz)\")\n    print(f\"On pan: {'Yes' if result['on_pan'] else 'No'}\")\n    print(f\"\\nExtracted parameters:\")\n    print(json.dumps(result['params'], indent=2))\n    \n    # Play the sample\n    display(ipd.Audio(result['audio'], rate=result['sample_rate']))\nexcept Exception as e:\n    print(f\"Could not download sample: {e}\")\n    print(\"You can upload your own WAV file using the button above\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "def download_note(note, filename=None):\n",
    "    \"\"\"Generate a note and download as WAV file.\"\"\"\n",
    "    update_params()\n",
    "    freq = get_frequency(note)\n",
    "    audio = generate_note(freq, params)\n",
    "    \n",
    "    # Convert to 16-bit stereo\n",
    "    audio_int = (audio * 32767).astype(np.int16)\n",
    "    stereo = np.column_stack((audio_int, audio_int))\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"{note.replace('#', 's')}.wav\"\n",
    "    \n",
    "    wavfile.write(filename, SAMPLE_RATE, stereo)\n",
    "    files.download(filename)\n",
    "    print(f\"‚úì Downloaded {filename}\")\n",
    "\n",
    "# Download a single note\n",
    "download_note('C5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_notes():\n",
    "    \"\"\"Generate and download all 29 tenor pan notes.\"\"\"\n",
    "    import zipfile\n",
    "    import os\n",
    "    \n",
    "    update_params()\n",
    "    \n",
    "    # Create temp directory\n",
    "    os.makedirs('sounds', exist_ok=True)\n",
    "    \n",
    "    for note in TENOR_PAN_NOTES:\n",
    "        freq = get_frequency(note)\n",
    "        audio = generate_note(freq, params)\n",
    "        audio_int = (audio * 32767).astype(np.int16)\n",
    "        stereo = np.column_stack((audio_int, audio_int))\n",
    "        \n",
    "        filename = f\"sounds/{note.replace('#', 's')}.wav\"\n",
    "        wavfile.write(filename, SAMPLE_RATE, stereo)\n",
    "        print(f\"  Generated {note}\")\n",
    "    \n",
    "    # Save params\n",
    "    with open('sounds/params.json', 'w') as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    \n",
    "    # Create zip\n",
    "    with zipfile.ZipFile('deepPan_sounds.zip', 'w') as zf:\n",
    "        for f in os.listdir('sounds'):\n",
    "            zf.write(f'sounds/{f}', f)\n",
    "    \n",
    "    files.download('deepPan_sounds.zip')\n",
    "    print(f\"\\n‚úì Downloaded deepPan_sounds.zip with {len(TENOR_PAN_NOTES)} notes\")\n",
    "\n",
    "# Uncomment to generate all notes:\n",
    "# download_all_notes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Links\n",
    "\n",
    "- [GitHub Repository](https://github.com/profLewis/deepPan)\n",
    "- [Browser Synthesizer](https://proflewis.github.io/deepPan/synth.html) (if GitHub Pages enabled)\n",
    "- [Interactive Player](https://proflewis.github.io/deepPan/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}